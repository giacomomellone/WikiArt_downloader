{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download high-resolution images from wikiArt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.parse import urljoin, urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid(url):\n",
    "    \"\"\"\n",
    "    Checks whether `url` is a valid URL.\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    return bool(parsed.netloc) and bool(parsed.scheme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_images(url):\n",
    "    \"\"\"\n",
    "    Returns all image URLs on a single `url`\n",
    "    \"\"\"\n",
    "    soup = bs(requests.get(url).content, \"html.parser\")\n",
    "    urls = []\n",
    "    for img in tqdm(soup.find_all(\"img\"), \"Extracting images\"):\n",
    "        img_url = img.attrs.get(\"src\")\n",
    "        if not img_url:\n",
    "            # if img does not contain src attribute, just skip\n",
    "            continue\n",
    "        # make the URL absolute by joining domain with the URL that is just extracted\n",
    "        img_url = urljoin(url, img_url)\n",
    "        # remove URLs like '/hsts-pixel.gif?c=3.2.5'\n",
    "        try:\n",
    "            pos = img_url.index(\"?\")\n",
    "            img_url = img_url[:pos]\n",
    "        except ValueError:\n",
    "            pass\n",
    "        # finally, if the url is valid\n",
    "        if is_valid(img_url):\n",
    "            img_url = img_url.split('!', 1)[0]\n",
    "            urls.append(img_url)\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_addresses(addresses):\n",
    "    \"\"\"\n",
    "    Returns all image URLs on a single `url`\n",
    "    \"\"\"\n",
    "    soup = bs(requests.get(addresses).content, \"html.parser\")\n",
    "    urls = []\n",
    "    \n",
    "    for img in tqdm(soup.findAll(\"li\", {\"class\": \"painting-list-text-row\"}), \"Extracting images\"):\n",
    "        img_url = img.a.get('href')\n",
    "        img_url = img_url.split('/', 2)[2]\n",
    "        img_url = \"https://uploads0.wikiart.org/images/\" + img_url + \".jpg\"\n",
    "        if not img_url:\n",
    "            # if img does not contain src attribute, just skip\n",
    "            continue\n",
    "        # make the URL absolute by joining domain with the URL that is just extracted\n",
    "        img_url = urljoin(addresses, img_url)\n",
    "        # remove URLs like '/hsts-pixel.gif?c=3.2.5'\n",
    "        try:\n",
    "            pos = img_url.index(\"?\")\n",
    "            img_url = img_url[:pos]\n",
    "        except ValueError:\n",
    "            pass\n",
    "        # finally, if the url is valid\n",
    "        if is_valid(img_url):\n",
    "            img_url = img_url.split('!', 1)[0]\n",
    "            urls.append(img_url)\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url, pathname):\n",
    "    \"\"\"\n",
    "    Downloads a file given an URL and puts it in the folder `pathname`\n",
    "    \"\"\"\n",
    "    # if path doesn't exist, make that path dir\n",
    "    if not os.path.isdir(pathname):\n",
    "        os.makedirs(pathname)\n",
    "    # download the body of response by chunk, not immediately\n",
    "    response = requests.get(url, stream=True)\n",
    "\n",
    "    # get the total file size\n",
    "    file_size = int(response.headers.get(\"Content-Length\", 0))\n",
    "\n",
    "    # get the file name\n",
    "    prefix = url.split('/', 5)[4]\n",
    "    prefix = prefix.split('.', 1)[0]\n",
    "    filename = os.path.join(pathname, prefix + \"_\" + url.split(\"/\")[-1])\n",
    "\n",
    "    # progress bar, changing the unit to bytes instead of iteration (default by tqdm)\n",
    "    progress = tqdm(response.iter_content(1024), \"Downloading \" + filename, total=file_size, unit=\"B\", unit_scale=True, unit_divisor=1024)\n",
    "    with open(filename, \"wb\") as f:\n",
    "        for data in progress:\n",
    "            # write data read to the file\n",
    "            f.write(data)\n",
    "            # update the progress bar manually\n",
    "            progress.update(len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(url, path):\n",
    "    # get all images\n",
    "    imgs = get_all_addresses(url)\n",
    "    for img in imgs:\n",
    "        # for each img, download it\n",
    "        download(img, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting images: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.wikiart.org/en/ad-reinhardt/all-works/text-list\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "artists_list = open(\"wikiart_artists.txt\", \"r\")\n",
    "lines = artists_list.readlines()\n",
    "\n",
    "for url_addr in range(len(lines)):\n",
    "    print(lines[url_addr].rstrip(\"\\n\")  + \"/all-works/text-list\")\n",
    "    main(lines[url_addr].rstrip(\"\\n\")  + \"/all-works/text-list\", \"wikiart_dataset\")\n",
    "    \n",
    "#main(\"https://www.wikiart.org/en/tsuruko-yamazaki/\", \"wikiart_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort the .txt file alphabetically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"wikiart_artists.txt\"\n",
    "\n",
    "f = open(txt , \"r\")\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "lines.sort()\n",
    "\n",
    "f = open(txt , \"w\")\n",
    "for line in lines:\n",
    "    f.write(line)\n",
    "    \n",
    "f.flush()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
